<div class="is-flex-tablet is-flex-direction-column is-justify-content-center is-align-items-center m-4">
  <section class="content column is-half" id="top">
    <h3 class="title has-text-centered"><%= link_to "back", root_path %></h3>
    <h3 class="subtitle has-text-centered"><%= link_to "bottom of page", "#bottom" %></h3>
    <h1 class="title has-text-centered">signal processing studies</h1>
    <h2 class="title is-5">5 aspects of Communcation System from Shannon's A Mathematical Theory of Communication:</h2>
    <ol>
      <li>an <strong>information source</strong> may be a sequence; or a single function of time f(t); a function of time and other variables f(x,y,t); two or more functions of time f(t), g(t), h(t); a vector field of several functions of several variables f(x,y,t), g(x,y,t), h(x,y,t); or various combinations of these</li>
      <li>a <strong>transmitter</strong> which produces a signal suitable for transmission (changing sound pressure, encoding operations like sampling, compression, quantization)</li>
      <li>a <strong>channel</strong> which is the medium over which the signal is sent (wires, cable, radio frequency band, beam of light)</li>
      <li>a <strong>receiver</strong> which usually performs the inverse operation that transmitter did</li>
      <li>a <strong>destination</strong> for the message</li>
    </ol>
    <h2 class="title is-5">some basics</h2>
    <p>
      time domain, frequency domain, spatial domain are the most common indepedent variables in a signal. the depedent variable can generally be called amplitude, while this domain can generally be called sample number.
    </p>
    <h3 class="subtitle is-5 is-italic">sample index is generated notated either:</h3>
      <ul>
        <li>1 to N</li>
        <li>0 to N-1 (more common for DSP)</li>
      </ul>
      <p>
        in electronics the mean is called the DC value. For AC, the signal is referred to from how much it fluctuates from the mean. if the signal is a simple waveform, it can be described simply by its peak to peak amplitude, but most signals have a random nature at which point we use standard deviation. 
      </p>
     <pre>
array = [set of numbers]
MEAN = array.sum() / array.size()
VARIANCE = SIGMA**2 =
    array.map(|e| e - MEAN).sum() ** 2 / array.size() - 1
    // the power of the fluctuation
SIGMA**2 =
    (array.map(|e| e**2).sum() - array.sum()**2 / (array.size()) / (N - 1)
STANDARD_DEVIATION = (SIGMA**2).sqrt() = SIGMA
    // how much a signal fluctuates
     </pre>
     <p>
     standard deviation only measures the AC component of a signal.
     rms (root-mean-squared) value measures both AC and DC components of a signal.
     </p>
     <pre>
// running stats are leaner computationally
sum = 0
sum_squares = 0
elements = [set of numbers]
RUNNING_STATS = SIGMA**2 = elements.each_with_index{|e,i|
    sum += e
    sum_squares += e**2
    mean = sum / i
    standard_dev = i.eql?(1) ? 0 : (Math.sqrt(sum_squares - sum**2/i)) / (i - 1)
    puts mean
    puts standard_dev
}
     </pre>
      <p>
      in some cases the mean describes what is being measured and the standard deviation represents noise. in these situations we describe the signal to noise ratio which is the mean divided by the standard deviation. another way to describe this is: <strong>coefficient of variation (CV)</strong> which is the <code>(standard deviation / mean) * 100%</code>
      </p>
      <p>
statistics is the science of interpreting numerical data, while probability is used in DSP to understand the processes that generate signals. a distiction between the acquired signal and the underlying process is key.
      </p>
      <pre>
SIGMA = STANDARD_DEVIATION
TYPICAL_ERROR = SIGMA / N**(1/2)
      </pre>
      <p>
      in probability theory the <strong>Strong Law of Large Numbers</strong> tells us that the error of a signal = 0 as N approaches infinity; so small number of samples will always have a larger error than larger number of samples.
      </p>
      <p class="is-italic">
      This concept of error is interesting when we think about what calculating a mean for a signal truly means. We are not calculating the mean for the underlying process, just the mean for N sampled signal, and we know this sampled signal will contain statistical noise. This is where the N-1 in the standard deviation comes from, this compenstation (compared to simply N) reduces the noise in small sample sizes. As N gets larger N-1 becomes more and more irrevelant (compared to N).
      </p>
      <p>
Calculating the mean and standard deviation can be computationally intensive. We can use a histogram to make this calculation more simplistic. To get a histogram we make a table where the index of the table is the number of possible values a sample can have. For an 8bit signal that value will be 256. The y-value of each x index is the number of times that sample value occurs in our signal.
      </p>
      <pre>
// histogram table
{ 
  sample_value1: # occurances in signal,
  sample_value2: # occurances in signal, 
  etc 
}
      </pre>
      <h4 class="title is-6">using Histogram <code>H</code></h4>
      <pre>
H = { table of values }

N = 0
for i=1,#H {
    N += H[i]
}
N = N

MEAN = 0
for i=1,#H {
    MEAN += H[i] * i
}
MEAN = MEAN/N

SD = 0
for i=1,#H {
    SD += H[i] * (i - MEAN)**2 
}
SD = SD / (N-1) 
      </pre>
      <p>
      <strong>pmf</strong> (probability mass function; for use on digital signals), <strong>pdf</strong> (probability density function; for use with analog signals) can be inferred from a histogram, as they are related concepts. If instead of the number of occurences (y-axis) you replace it with the probability of occurences: you go from the histogram to the <strong>pmf</strong>. the sum of values of the histogram is equal the number of samples, the sum of values of the <strong>pmf</strong> is equal to 1. the sum of the area under the curve of the <strong>pdf</strong> is equal to 1.
      </p>
      <ul>
        <li>the <strong>pdf</strong> of a few common signals: square wave: two infinitesimally small spikes representing the two possible values. triangle wave: will have a uniform distribution between its peak to peak values. random noise: so-called normal distribution or gaussian bell curve.</li>
        <li>the integral of the pdf (called the cumulative distribution function <strong>cdf</strong> or <code>psi(x)</code> where <code>x</code> is the standard deviation) is used to find the probability that a signal will be within a certain range.</li>
      </ul>
      <pre>
fn binned_histogram(signal: [f64;25000]) -> [i64;999] {
    // used when number of possible values > number of samples
    // particulary when using floating point numbers

    // choosing the number of bins is balancing act
    // between x-axis and y-axis resolution.

    // let signal have possible values between 0.0 and 10.0

    let bins: [i64;999] = [0;999];
    signal.iter().for_each( |e|
        let bin_number = (e * 100.0) as usize;
        bins[bin_number] += 1;
    );
    bins
}
      </pre>
      <pre>
fn raw_exponential_curve(x: f64) -> f64 {
    // random processes tend to have a bell shaped distribution
    (-x.powi(2)).exp()
}

fn normal_distrubtion(x:f64) -> f64 {
    // normalized gaussian distribution
    let num: f64 = (-(x - MEAN).powi(2) /
        (2.0 * STANDARD_DEVIATION.powi(2))).exp();
    let den: f64 = STANDARD_DEVIATION * (2.0 * PI).sqrt();
    num / den
}
      </pre>
      <p>
Gaussian curves cannot be integrated by elementary methods. So the integral is calculated via numerical integration. You sample the continuous curve and add these samples up simulate integration.
      </p>
      <p>
In order to test DSP with signals that resemble real-world signals, we have a need to generate random noise signals that have a Gaussian `pdf`. The randomness of a standard random number generator has a uniform distribution. If we create a random number by adding two random
      </p>
      <pre>
-- random number generation
-- needed for creating test signals
-- that resemble the gaussian pdf of real-world signals

function uniform_distribution
    -- mean = 0.5, standard_dev = 1/SQRT(12)
    return math.random
end

function less_uniform_distribution
    -- mean = 1; standard_dev = 1/SQRT(6)
    return math.random + math.random
end

function nearly_gaussian_distribution
    -- mean = 6.0; standard_dev = 1
    sum = math.random
    for _=1,12 do
        sum += math.random
    end
    return sum
end

function seed_to_random_algo(seed)
    -- seed will be a 1 or 0
    -- a,b,c are tuned constants; how are they chosen?
    return (a * seed + b) % c
end
      </pre>
      <p class="has-text-centered">
      an audio EQ analogy: the <strong>accuracy</strong> is the frequency, <strong>precision</strong> is the Q
      </p>
      <h2 class="title is-5">bases</h2>
      <ul>
        <li>logarthmic base 2; binary digits; bits - a device with two stable positions (relay or flip/flop) can store one bit of information. N such devices can stsore N bits.</li>
        <li>logarthmic base 10; decimal digits - 1 decimal digit is about 3 1/3 bits; a device with 10 stable positions stores one decimal digit.</li>
        <li>logarthimic base e; natural units - useful for integration and differentiation</li>
        <li>change from base a to base b requires multiplying by log (base b) a</li>
        <li>the entropy for a signal character message using the alphabet (a-z) is = log base 2 * 26 = 4.7</li>
      </ul>
      <h2 class="title is-5">digital circuits</h2>
      <pre>
fn half_adder(x: bool, y: bool) -> (bool,bool) {
    let sum = x&y;
    let carry = x^y;
    (sum, carry)
}
      </pre>
      <pre>
fn full_adder(x: bool, y: bool, c: bool) -> (bool, bool) {
    let (a, b) = half_adder(x, y);
    let (j, k) = half_adder(b, c);

    let sum = a || j;
    let carry = k;

    (sum,carry)
}
      </pre>
      <pre>
fn bitwise_addition(x: u8, y: u8) -> u8 {
    if y == 0 { x } else { add(x^y, (x&y) << 1 ) }
}
      </pre>
      <h2 class="title is-5">analog to digital / digital to analog converters</h2>
      <ul>
        <li>signal to s&h to adc to digitized signal</li>
        <li>the s&h holds the signal at a value while the adc makes the conversion</li>
        <ul>
          <li>s&h = <strong>sampling</strong>: taking the continuous indepedent variable (time) and make it discrete</li>
          <li>adc = <strong>quantization</strong>: taking the continuous depedent variable (volts) and make it discrete; rounding to nearest integer</li>
          <li>each sample can have an error of +/- 1/2 LSB (least significant bit = the distance between quantization steps)</li>
        </ul>

        <li>quantization errors can be found by subtracting the sampled signal from the quantized signal (this generally looks like random noise)
        </li>
        <ul>
          <li>the digitized signal can be thought of as being the original analog signal plus the quantization error; which is wild</li>
          <li>quantization = the addition of a specific amount of random noise to the signal.</li>
        </ul>
        <li>this additive noise has a uniform distribution, a mean of 0, and a standard deviation of <code>1/SQRT(12)</code></li>
        <ul>
          <li>passing a signal through an 8 bit digitizer adds an rms noise of <code>(1/SQRT(12))/256</code></li>
          <li>passing a signal through an 12 bit digitizer adds an rms noise of <code>(1/SQRT(12))/4096</code></li>
          <li>passing a signal through an 16 bit digitizer adds an rms noise of <code>(1/SQRT(12))/65536</code></li>
        </ul>
        <li>thus the number of bits determines the precision of the data</li>
      </ul>
      <ul>
        <li>a 1 volt signal digitized to 8 bits with a random noise of 1.0 millivolts</li>
        <li>1 volt becomes 255; 1 mV becomes .255 LSB</li>
      </ul>
      <h3 class="subtitle is-6">how many bits are needed in a system?</h3>
      <ol>
        <li>how much noise is already present in the system?</li>
        <li>how much noise can be tolerated in the digital signal?</li>
      </ol>

      <h2 class="title is-5">dithering</h2>
      <p>
      a technique for when this model for quantization fails (when you have slowly moving analog signals that varies less than +/- 1/2 LSB); this quantization error tends to look less like additive noise and more like a distortion or thresholding effect
      </p>
      <p>
in dithering we add a small amount of random noise to the analog signal; the added noise causes the digitized signal (which otherwise may have been stuck at certain quanitization levels) to toggle between adjacent quantization levels which provides more information about the original signal
      </p>
      <p class="is-italic has-text-centered">if you can reconstruct the original signal from the sample signal, you have sampled properly.</p>
      <h2 class="title is-5">shannon/nyquist sampling theorem</h2>
      <p>
a continuous signal can be properly sampled only if it doesnt contain any frequency components above 1/2 the sampling rate.
      </p>
      <p>
      a sample rate of 2000 samples per second requires the original signal to be made up of frequencies below 1000 cyles per second. Any frequencies above this will be aliased into frequencies between 0-1000 cycles per second. There can also be a 180 degree phase shift.
      </p>
      <h3 class="subtitle is-italic is-6">four important frequencies for when sampling a signal (generally and interchangeably known as the Nyquist frequecy or Nyquist rate)</h3>
      <ol>
        <li>the highest frequency of the signal</li>
        <li>the highest frequency of the signal * 2</li>
        <li>the sampling rate</li>
        <li>the sampling rate * 1/2</li>
      </ol>
      <h2 class="title is-5">impulse train</h2>
      <p>
      a technique that will be used to turn the discrete digitized signal into a continuous one so that we can make effective comparisons between our array of numbers and the original signal.
      </p>
      <h2 class="title is-5">DFT</h2>
      <p>
      The k'th spectral sample is given by the dot product of the signal X with the k'th sinuosid. The k'th sinusoid is given by <code>exp^(j * 2 * pi * n * k / N)</code>
      </p>
      <pre>
import numpy as np
import matplotlib.pyplot as plt

N = 64 # size
k0 = 7 # periodicity / frequency
x = np.cos(2 * np.pi * k0 / N * np.arange(N))

X = np.array([])
nv = np.arange(-N/2, N/2) # time index
kv = np.arange(-N/2, N/2) # frequency index

## DFT

for k in kv:
    s = np.exp(1j * 2 * np.pi * k / N * nv)
    X = np.append(X, sum(x*np.conjugate(s)))

plt.plot(kv, abs(X))
plt.axis([-N/2, N/2 - 1, 0, N])
plt.show()

## Inverse DFT

y = np.array([])
for n in nv:
    s = np.exp(1j * 2 * np.pi * n / N * kv)
    y = np.append(y, 1.0/N * sum(X*s))

plt.plot(kv, y)
plt.axis([-N/2, N/2 - 1, -1, 1])
plt.show()
      </pre>
      <h2 class="title is-5">Fourier Transform Properties</h2>
      <ul>
        <li><h3>linearity</h3> adding signals in the time domain corresponds to adding signals in the spectral domain</li>
        <li><h3>shift</h3> shifting samples of a signal in the time domain corresponds to a change in the phase of the spectrum, but no change in the magnitude</li>
        <li><h3>symmetry</h3><ul>
            <li> for real signals:</li>
            <ul>
              <li>the real part of the signal (magnitude) will have even symmetry</li>
              <li>the imaginary part of the signal (phase) will have odd symmetry</li>
            </ul>
            <li>for real signals with even symmerty</li>
            <ul>
              <li>the real part of the signal (magnitude) will have even symmetry</li>
              <li>the imaginary part of the signal (phase) will be a multiple of pi or 0</li>
            </ul>
          </ul>
          <li><h3>convolution</h3> if we convolve two time domain signals it corresponds to the product of signals in the frequency domain</li>
          <li><h3>energy conservation</h3> the energy of a signal in the time domain is the same as the energy of a signal in the frequency domain</li>
          <li><h3>amplitude</h3> we can convert the magnitude X of a DFT to dB <code>= 20 * log[base10](abs(X))</code></li>
          <li><h3>phase unwrapping</h3> in the phase spectrum, anywhere there is a discontinuity (which is anytime we are going oscillating back around on a cartesian plane) we can <strong>add 2pi</strong> to the signal. A way to represent the phase spectrum of a DFT in a way that is easier to vizualize.</li>
          <li><h3>zero padding</h3> adding continuous zeros to a signal in one domain corresponds to interpolation of the signal in the other domain.</li>
          <li><h3>fast fourier transform</h3> speeds up the transform. Cooley-Tukey algorithm requires an input signal that is of power of 2. It takes advantage of the symmetrical nature of these signals to recursively break the signal down by <code>N/2</code> until left with pairs. It then performs the calculation on those pairs before adding their Spectrum back together.</li>
          <li><h3>zero-phase windowing</h3>; in order to take a signal (from <code>-N/2 - N/2</code>) and prepare it for FFT we apply Zero padding and zero-phase windowing to what is called the FFT Buffer (which will be represented <code>0 - (N + number of samples to make it a power of 2)</code>. Zero phase windowing takes the Zero Phase point and all the positive values of the signal and puts that the beginning of the FFT buffer. We take all the negative values and place them at the end of the FFT buffer. We zero pad any empty space in between the signals. We can visualize this as phase shifted signal in a circular buffer.</li>
          <pre>
import numpy as np
from scipy.signal import triang
from scipy.fftpack import fft

x = triang(15)
    # 15 is the number of samples
fftbuffer = np.zeros(15)
    # create our buffer, 15 zeros
fftbuffer[:8] = x[7:]
    # the beginning of the buffer
    # will be the end of the triangle
fftbuffer[8:] = x[:7]
    # vice versa

X = fft(fftbuffer)
    # this implementation can deal with number of samples
    # that are not a power of two
mX = abs(X)
    # magnitude
pX = np.angle(X)
    # phase

plot(x) # triangular function
plot(fftbuffer) # 0 indexed function
plot(X) # spectrum
plot(mX) # magnitude
plot(pX) # phase w/ zero-phase windowing
          </pre>
          <li><h3>analysis/synthesis</h3> Input Signal to FFT = (magnitude, phase) to Inverse FFT = Output Signal (Input == Output)</li>
          <pre>
          import numpy as np
import math
from scipy.fftpack import fft, ifft

# FFT size(N) must be a power of two
# window size(w) must be smaller than N
# input signal(x); returns output signal(y)

def dft(x, w, N):
    hN = (N//2)+1
        # size of positive spectrum, it includes sample 0
    hM1 = (w.size+1)//2
        # half analysis window size by rounding
    hM2 = int(math.floor(w.size/2))
        # half analysis window size by floor
    fftbuffer = np.zeros(N)
        # initialize buffer for FFT
    y = np.zeros(x.size)
        # initialize output array

    # analyze
    xw = x*w
        # window the input sound
    fftbuffer[:hM1] = xw[hM2:]
    fftbuffer[-hM2:] = xw[:hM2]
        # zero-phase window in fftbuffer
    X = fft(fftbuffer)
    absX = abs(X[:hN])
    mX = 20 * np.log10(absX)
        # magnitude spectrum of positive frequencies in dB
    pX = np.unwrap(np.angle(X[:hN]))
        # unwrapped phase spectrum of positive frequencies

    # synthesize
    Y = np.zeros(N, dtype = complex)
        # clean output spectrum
    Y[:hN] = 10**(mX/20) * np.exp(1j*pX)
        # generate positive frequencies
    Y[hN:] = 10**(mX[-2:0:-1]/20) * np.exp(-1j*pX[-2:0:-1])
        # generate negative frequencies
    fftbuffer = np.real(ifft(Y))
        # compute inverse FFT
    y[:hM2] = fftbuffer[-hM2:]
    y[hM2:] = fftbuffer[:hM1]
        # undo zero-phase window
    return y
    <pre>
  </section>
  <div id="bottom"></div>
  <h3 class="title has-text-centered"><%= link_to "top of page", "#top" %></h3>
</div>
